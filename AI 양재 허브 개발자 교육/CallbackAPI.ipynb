{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14134be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Copyright 2020 The TensorFlow Authors.\n",
    "\n",
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "# Callback API 를 이용한 학습 흐름 제어\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/guide/keras/train_and_evaluate\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">TensorFlow.org에서 보기</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/ko/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Google Colab에서 실행</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/ko/guide/keras/train_and_evaluate.ipynb\">     <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">    GitHub에서 소스 보기</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/ko/guide/keras/train_and_evaluate.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">노트북 다운로드</a></td>\n",
    "</table>\n",
    "\n",
    "## 시작하기\n",
    "\n",
    "콜백은 훈련, 평가 또는 추론 중에 Keras 모델의 동작을 사용자 정의할 수 있는 강력한 도구입니다. TensorBoard로 훈련 진행 상황과 결과를 시각화하기 위한 `tf.keras.callbacks.TensorBoard` 또는 훈련 도중 모델을 주기적으로 저장하는 `tf.keras.callbacks.ModelCheckpoint` 등이 여기에 포함됩니다.\n",
    "\n",
    "## 설정\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data (these are NumPy arrays)\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "y_train = y_train.astype(\"float32\")\n",
    "y_test = y_test.astype(\"float32\")\n",
    "\n",
    "# Reserve 10,000 samples for validation\n",
    "x_val = x_train[-10000:]\n",
    "y_val = y_train[-10000:]\n",
    "x_train = x_train[:-10000]\n",
    "y_train = y_train[:-10000]\n",
    "\n",
    "나중에 재사용하기 위해 모델 정의 함수 생성\n",
    "\n",
    "def get_compiled_model():\n",
    "\n",
    "    inputs = keras.Input(shape=(784,), name=\"digits\")\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_1\")(inputs)\n",
    "    x = layers.Dense(64, activation=\"relu\", name=\"dense_2\")(x)\n",
    "    outputs = layers.Dense(10, activation=\"softmax\", name=\"predictions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=\"rmsprop\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"sparse_categorical_accuracy\"],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "## 콜백 사용하기\n",
    "\n",
    "Keras의 콜백은 훈련 중 다른 시점(epoch의 시작, 배치의 끝, epoch의 끝 등)에서 호출되며 다음과 같은 동작을 구현하는 데 사용할 수 있는 객체입니다.\n",
    "\n",
    "- 훈련 중 서로 다른 시점에서 유효성 검사 수행(내장된 epoch당 유효성 검사에서 더욱 확장)\n",
    "- 정기적으로 또는 특정 정확도 임계값을 초과할 때 모델 검사점 설정\n",
    "- 훈련이 정체 된 것처럼 보일 때 모델의 학습 속도 변경\n",
    "- 훈련이 정체 된 것처럼 보일 때 최상위 레이어의 미세 조정\n",
    "- 교육이 종료되거나 특정 성능 임계 값을 초과 한 경우 전자 메일 또는 인스턴트 메시지 알림 보내기\n",
    "- 기타\n",
    "\n",
    "콜백은 `fit()` 에 대한 호출에 목록으로 전달 될 수 있습니다.\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(   #오버피팅이 발생하기 전에 학습을 끝내기 위해 사용\n",
    "        # Stop training when `val_loss` is no longer improving\n",
    "        monitor=\"val_loss\",\n",
    "        # \"no longer improving\" being defined as \"no better than 1e-2 less\"\n",
    "        min_delta=1e-2,\n",
    "        # \"no longer improving\" being further defined as \"for at least 2 epochs\"\n",
    "        patience=2,  #1번째까지 봐줌, 2번째 발생 시 stop \n",
    "        verbose=1,   \n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    callbacks=callbacks,\n",
    "    validation_data = (x_val, y_val),\n",
    ")\n",
    "\n",
    "### 많은 내장 콜백을 사용할 수 있습니다\n",
    "\n",
    "- `ModelCheckpoint` : 주기적으로 모델을 저장\n",
    "- `EarlyStopping`: 훈련이 더 이상 유효성 검사 메트릭을 개선하지 못하는 경우 훈련을 중단\n",
    "- `TensorBoard` : 학습 과정을 시각확 할 수 있는 TensorBoard에서 시각화 할수 있는 학습 이력 정보 생성\n",
    "- `CSVLogger` : 손실 및 메트릭 데이터를 CSV 파일로 저장\n",
    "- 기타\n",
    "\n",
    "## ModelCheckpoint\n",
    "\n",
    "상대적으로 큰 데이터세트에 대한 모델을 훈련시킬 때는 모델의 ModelCheckpoint 빈번하게 저장하는 것이 중요합니다.\n",
    "\n",
    "이를 수행하는 가장 쉬운 방법은 `ModelCheckpoint` 콜백을 사용하는 것입니다.\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(   #특정 시점의 가중치만을 저장\n",
    "        # Path where to save the model\n",
    "        # The two parameters below mean that we will overwrite\n",
    "        # the current checkpoint if and only if\n",
    "        # the `val_loss` score has improved.\n",
    "        # The saved model name will include the current epoch.\n",
    "        filepath=\"mymodel_{epoch}\",   #무슨 이름으로 저장할지 정함\n",
    "        save_best_only=True,  # Only save a model if `val_loss` has improved.\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "    )\n",
    "]\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=2, batch_size=64, callbacks=callbacks, validation_data= (x_val, y_val)\n",
    ")\n",
    "\n",
    "## 학습 속도 일정 사용하기\n",
    "\n",
    "딥 러닝 모델을 훈련 할 때 일반적인 패턴은 훈련이 진행됨에 따라 점차적으로 학습을 줄이는 것입니다. 이것을 일반적으로 \"학습률 감소\"라고합니다.\n",
    "\n",
    "learning rate 감소 스케줄은 정적(현재 에포크 또는 현재 배치 인덱스의 함수로서 미리 고정됨) 또는 동적(모델의 현재 행동, 특히 검증 손실에 대응) 일 수있다.\n",
    "\n",
    "### 1). 옵티마이저로 schedule 전달하기\n",
    "\n",
    "옵티 마이저에서 schedule 객체를 `learning_rate` 인수로 전달하여 정적 학습 속도 감소 스케줄을 쉽게 사용할 수 있습니다.\n",
    "\n",
    "initial_learning_rate = 0.1\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(  # ExponentialDecay는 계속 같은 비율로 learning rate를 감소시켜준다.\n",
    "    initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    ")\n",
    "\n",
    "optimizer = keras.optimizers.RMSprop(learning_rate=lr_schedule)\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"sparse_categorical_accuracy\"],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=4, batch_size=1, validation_split=0.2\n",
    ")\n",
    "\n",
    "`ExponentialDecay` , `PiecewiseConstantDecay` , `PolynomialDecay` 및 `InverseTimeDecay` 와 같은 몇 가지 기본 제공 일정을 사용할 수 있습니다.\n",
    "\n",
    "### 2. 콜백을 사용하여 동적 learning rate schedule 구현\n",
    "\n",
    "옵티마이저가 validation 데이터 메트릭에 액세스할 수 없으므로 동적 학습률 schedule(예: validation 데이터를 이용한 loss값이 더 이상 개선되지 않을 때 학습률 감소)을 구현 할 수 없습니다.\n",
    "\n",
    "그러나 콜백은 validation 데이터 메트릭을 포함해 모든 메트릭에 액세스할 수 있습니다.\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ReduceLROnPlateau(  #learning rate 감소, 특정한 값만큼 연속적으로 작업이 일어날때 사용\n",
    "      monitor='val_loss',\n",
    "      factor=0.1,    #learning rate 을 줄이기 위해 epoch마다 0.1만큼 곱해줌\n",
    "      patience=2,\n",
    "      cooldown=1,\n",
    "      verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")\n",
    "\n",
    "## 훈련 중 손실 및 메트릭 시각화하기\n",
    "\n",
    "교육 중에 모델을 주시하는 가장 좋은 방법은 로컬에서 실행할 수있는 브라우저 기반 응용 프로그램 인 [TensorBoard](https://www.tensorflow.org/tensorboard) 를 사용하는 것입니다.\n",
    "\n",
    "- 교육 및 평가를위한 손실 및 지표의 라이브 플롯\n",
    "- (옵션) 레이어 활성화 히스토그램 시각화\n",
    "- (옵션) `Embedding` 레이어에서 학습한 포함된 공간의 3D 시각화\n",
    "\n",
    "pip와 함께 TensorFlow를 설치한 경우, 명령줄에서 TensorBoard를 시작할 수 있습니다.\n",
    "\n",
    "```\n",
    "tensorboard --logdir=/full_path_to_your_logs\n",
    "```\n",
    "\n",
    "### TensorBoard 콜백 사용하기\n",
    "\n",
    "TensorBoard를 Keras 모델 및 fit 메서드와 함께 사용하는 가장 쉬운 방법은 `TensorBoard` 콜백입니다.\n",
    "\n",
    "가장 간단한 경우로, 콜백에서 로그를 작성할 위치만 지정하면 바로 쓸 수 있습니다.\n",
    "\n",
    "import datetime\n",
    "\n",
    "# 학습데이터의 log를 저장할 폴더 생성 (지정)\n",
    "log_dir = \"./logs/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "%load_ext tensorboard\n",
    "\n",
    "%tensorboard --logdir {log_dir}\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "  keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "]\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")\n",
    "\n",
    "# 자신만의 콜백 작성하기\n",
    "\n",
    "## 콜백 메서드의 개요\n",
    "\n",
    "### 전역 메서드\n",
    "\n",
    "#### `on_(train|test|predict)_begin(self, logs=None)`\n",
    "\n",
    "`fit`/`evaluate`/`predict` 시작 시 호출됩니다.\n",
    "\n",
    "#### `on_(train|test|predict)_end(self, logs=None)`\n",
    "\n",
    "`fit`/`evaluate`/`predict` 종료 시 호출됩니다.\n",
    "\n",
    "### 훈련/테스트/예측을 위한 배치 레벨의 메서드\n",
    "\n",
    "#### `on_(train|test|predict)_batch_begin(self, batch, logs=None)`\n",
    "\n",
    "훈련/테스트/예측 중에 배치를 처리하기 직전에 호출됩니다.\n",
    "\n",
    "#### `on_(train|test|predict)_batch_end(self, batch, logs=None)`\n",
    "\n",
    "훈련/테스트/예측이 끝날 때 호출됩니다. 이 메서드에서 `logs`는 메트릭 결과를 포함하는 dict입니다.\n",
    "\n",
    "### 에포크 레벨 메서드(훈련만 해당)\n",
    "\n",
    "#### `on_epoch_begin(self, epoch, logs=None)`\n",
    "\n",
    "훈련 중 epoch가 시작될 때 호출됩니다.\n",
    "\n",
    "#### `on_epoch_end(self, epoch, logs=None)`\n",
    "\n",
    "훈련 중 epoc가이 끝날 때 호출됩니다.\n",
    "\n",
    "## 기본적인 예제\n",
    "\n",
    "다음의 경우 로깅하는 간단한 사용자 정의 콜백을 정의합니다.\n",
    "\n",
    "- `fit`/`evaluate`/`predict`가 시작하고 끝날 때\n",
    "- 각 에포크가 시작하고 끝날 때\n",
    "- 각 훈련 배치가 시작하고 끝날 때\n",
    "- 각 평가(테스트) 배치가 시작하고 끝날 때\n",
    "- 각 추론(예측) 배치가 시작하고 끝날 때\n",
    "\n",
    "class myCallback(tf.keras.callbacks.Callback):     \n",
    "  def on_epoch_end(self, epoch, logs={}):   #epoch이 끝날때마다 실행되게 함\n",
    "       if logs.get('sparse_categorical_accuracy') > 0.6:\n",
    "         print(\"blahblah\")\n",
    "         self.model.stop_training = True   #모델의 accuracy가 60퍼가 넘으면 학습 멈춤\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "callbacks = [\n",
    "             myCallback()\n",
    "]   #꼭 리스트에 담을 필요는 없음\n",
    "\n",
    "model.fit(\n",
    "    x_train, y_train, epochs=20, batch_size=64, callbacks=callbacks, validation_split=0.2\n",
    ")\n",
    "\n",
    "class CustomCallback(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Starting training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop training; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"End epoch {} of training; got log keys: {}\".format(epoch, keys))\n",
    "\n",
    "    def on_test_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_test_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop testing; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_begin(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Start predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_predict_end(self, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"Stop predicting; got log keys: {}\".format(keys))\n",
    "\n",
    "    def on_train_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Training: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Evaluating: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_begin(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: start of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "    def on_predict_batch_end(self, batch, logs=None):\n",
    "        keys = list(logs.keys())\n",
    "        print(\"...Predicting: end of batch {}; got log keys: {}\".format(batch, keys))\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=1,\n",
    "    verbose=0,\n",
    "    validation_split=0.5,\n",
    "    callbacks=[CustomCallback()],\n",
    ")\n",
    "\n",
    "res = model.evaluate(\n",
    "    x_test, y_test, batch_size=128, verbose=0, callbacks=[CustomCallback()]\n",
    ")\n",
    "\n",
    "res = model.predict(x_test, batch_size=128, callbacks=[CustomCallback()])\n",
    "\n",
    "### `logs` dict 사용법\n",
    "\n",
    "`logs` dict에는 손실값과 배치 또는 에포크의 끝에 있는 모든 메트릭이 포함됩니다.\n",
    "\n",
    "class LossAndErrorPrintingCallback(keras.callbacks.Callback):\n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_test_batch_end(self, batch, logs=None):\n",
    "        print(\"For batch {}, loss is {:7.2f}.\".format(batch, logs[\"loss\"]))\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(\n",
    "            \"The average loss for epoch {} is {:7.2f} \"\n",
    "            \"and mean absolute error is {:7.2f}.\".format(\n",
    "                epoch, logs[\"loss\"], logs[\"sparse_categorical_accuracy\"]\n",
    "            )\n",
    "        )\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=128,\n",
    "    epochs=2,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback()],\n",
    ")\n",
    "\n",
    "## Keras 콜백 애플리케이션의 예\n",
    "\n",
    "### 최소 손실 시 조기 중지\n",
    "\n",
    "이 첫 번째 예는 `self.model.stop_training` (boolean) 속성을 설정하여 최소 손실에 도달했을 때 훈련을 중단하는 `Callback`을 생성하는 방법을 보여줍니다. 선택적으로, 로컬 최소값에 도달한 후 중단하기 전에 기다려야 하는 에포크 수를 지정하는 인수 `patience`을 제공할 수 있습니다.\n",
    "\n",
    "`tf.keras.callbacks.EarlyStopping`은 더 완전한 일반적인 구현을 제공합니다.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class EarlyStoppingAtMinLoss(keras.callbacks.Callback):   #Callback 클래스 상속받음\n",
    "    \"\"\"Stop training when the loss is at its min, i.e. the loss stops decreasing.\n",
    "\n",
    "  Arguments:\n",
    "      patience: Number of epochs to wait after min has been hit. After this\n",
    "      number of no improvement, training stops.\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, patience=0):     #initialize(초기화) 함수\n",
    "        super(EarlyStoppingAtMinLoss, self).__init__()   #부모클래스에 접근(super), 부모 클래스 초기화\n",
    "        self.patience = patience   #멤버변수로 등록\n",
    "        # best_weights to store the weights at which the minimum loss occurs.\n",
    "        self.best_weights = None   #최고의 가중치 값들을 저장하기 위한 멤버 변수\n",
    "\n",
    "    def on_train_begin(self, logs=None):   #training 시작때 호출됨\n",
    "        # The number of epoch it has waited when loss is no longer minimum.\n",
    "        self.wait = 0   #몇 번 patient 가 일어났는지 저장\n",
    "        # The epoch the training stops at.\n",
    "        self.stopped_epoch = 0\n",
    "        # Initialize the best as infinity.\n",
    "        self.best = np.Inf     #무한대값으로 초기화\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):    #epoch이 끝날때 호출됨\n",
    "        current = logs.get(\"loss\")   \n",
    "        if np.less(current, self.best):  #무한대값은 연산자로 비교 불가, less함수 사용\n",
    "            self.best = current #current가 best보다 작으므로, best를 current로 바꿔줌\n",
    "            self.wait = 0  \n",
    "            # Record the best weights if current results is better (less).\n",
    "            self.best_weights = self.model.get_weights()   #모델에 접근해서 get_weights함수를 이용해서 최고의 weight를 업데이트함\n",
    "        else:  #반대 경우--> 성능이 나빠지는 경우 학습을 중단시키는 작업\n",
    "            self.wait += 1     \n",
    "            if self.wait >= self.patience:   #patience만큼  wait가 올라가게 되면 중단을 시키는 것\n",
    "            #최소 손실 시 중지 코드에서 patience는 총 횟수가 아니라 연속으로 나온 횟수\n",
    "\n",
    "  \n",
    "                self.stopped_epoch = epoch\n",
    "                self.model.stop_training = True   #멈춰! \n",
    "                print(\"Stop Training\")\n",
    "                self.model.set_weights(self.best_weights)  #멈친 상태의 모델의 가중치는 베스트가 아니기 때문에 기존에 저장해둔 best_weights를 모델의 weight로 설정  \n",
    "\n",
    "    def on_train_end(self, logs=None):   #학습이 끝날 때 epoch이 끝났음을 알려주는 문구 출력\n",
    "        if self.stopped_epoch > 0:\n",
    "            print(\"Epochs : [] training stopped\" .format(self.stopped_epoch))\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=30,\n",
    "    verbose=0,\n",
    "    callbacks=[LossAndErrorPrintingCallback(), EarlyStoppingAtMinLoss(2)],\n",
    ")\n",
    "\n",
    "### 학습 속도 스케줄링\n",
    "\n",
    "이 예제에서는 사용자 정의 콜백을 사용하여 훈련 동안 옵티마이저의 학습 속도를 동적으로 변경하는 방법을 보여줍니다.\n",
    "\n",
    "보다 일반적인 구현에 대해서는 `callbacks.LearningRateScheduler`를 참조하세요.\n",
    "\n",
    "class CustomLearningRateScheduler(keras.callbacks.Callback):\n",
    "    \"\"\"Learning rate scheduler which sets the learning rate according to schedule.\n",
    "\n",
    "  Arguments:\n",
    "      schedule: a function that takes an epoch index\n",
    "          (integer, indexed from 0) and current learning rate\n",
    "          as inputs and returns a new learning rate as output (float).\n",
    "  \"\"\"\n",
    "\n",
    "    def __init__(self, schedule):   \n",
    "        super(CustomLearningRateScheduler, self).__init__()  #부모클래스 불러와서 초기화\n",
    "        self.schedule = schedule    #전달받은 schedule함수를 멤버 변수로(함수형...생소하네ㅠ) \n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):  #learning rate 떨어뜨리는 작업\n",
    "        if not hasattr(self.model.optimizer, \"lr\"):  \n",
    "            raise ValueError('Optimizer must have a \"lr\" attribute.')\n",
    "        # Get the current learning rate from model's optimizer.\n",
    "        lr = float(tf.keras.backend.get_value(self.model.optimizer.learning_rate))  #모델에 있는 learning rate값을 가져옴 tf.keras.backend.get_value()\n",
    "        # Call schedule function to get the scheduled learning rate.\n",
    "        scheduled_lr = self.schedule(epoch, lr)\n",
    "        # Set the value back to the optimizer before this epoch starts\n",
    "        tf.keras.backend.set_value(self.model.optimizer.lr, scheduled_lr)\n",
    "        print(\"\\nEpoch %05d: Learning rate is %6.4f.\" % (epoch, scheduled_lr))  #잘 변경되었는지 확인하는 log 찍기\n",
    "\n",
    "\n",
    "LR_SCHEDULE = [\n",
    "    # (epoch to start, learning rate) tuples\n",
    "    (3, 0.05),\n",
    "    (6, 0.01),\n",
    "    (9, 0.005),\n",
    "    (12, 0.001),\n",
    "]\n",
    "\n",
    "\n",
    "def lr_schedule(epoch, lr):  \n",
    "    \"\"\"Helper function to retrieve the scheduled learning rate based on epoch.\"\"\"\n",
    "    if epoch < LR_SCHEDULE[0][0] or epoch > LR_SCHEDULE[-1][0]:\n",
    "        return lr\n",
    "    for i in range(len(LR_SCHEDULE)):\n",
    "        if epoch == LR_SCHEDULE[i][0]:\n",
    "            return LR_SCHEDULE[i][1]\n",
    "    return lr\n",
    "\n",
    "del model\n",
    "\n",
    "model = get_compiled_model()\n",
    "model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=64,\n",
    "    steps_per_epoch=5,\n",
    "    epochs=15,\n",
    "    verbose=0,\n",
    "    callbacks=[\n",
    "        LossAndErrorPrintingCallback(),\n",
    "        CustomLearningRateScheduler(lr_schedule),\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
